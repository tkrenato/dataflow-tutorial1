{
  "cells": [
   {
    "cell_type": "markdown",
    "id": "c88f2e64-3160-41b3-b346-9b7f93ba3c0e",
    "metadata": {},
    "source": [
     "##### Copyright 2022 Google Inc.\n",
     "\n",
     "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
     "<!--\n",
     "    Licensed to the Apache Software Foundation (ASF) under one\n",
     "    or more contributor license agreements.  See the NOTICE file\n",
     "    distributed with this work for additional information\n",
     "    regarding copyright ownership.  The ASF licenses this file\n",
     "    to you under the Apache License, Version 2.0 (the\n",
     "    \"License\"); you may not use this file except in compliance\n",
     "    with the License.  You may obtain a copy of the License at\n",
     "\n",
     "      http://www.apache.org/licenses/LICENSE-2.0\n",
     "\n",
     "    Unless required by applicable law or agreed to in writing,\n",
     "    software distributed under the License is distributed on an\n",
     "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
     "    KIND, either express or implied.  See the License for the\n",
     "    specific language governing permissions and limitations\n",
     "    under the License.\n",
     "-->\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "dc51b0cb-57bd-41ce-b8d8-5174587d011e",
    "metadata": {},
    "source": [
     "# Interactively run pipelines at scale with FlinkRunner on notebook-managed clusters\n",
     "\n",
     "This notebook demonstrates how to run Beam pipelines with a FlinkRunner hosted on a notebook-managed [Cloud Dataproc](https://cloud.google.com/dataproc) cluster ([Learn more](https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#interactive_flinkrunner_on_notebook-managed_clusters)). The feature enables working with production sized data using thousands of parallel processes from the notebook because workers are distributed to a Google Cloud internal cluster instead of on the single notebook GCE instance itself.\n",
     "\n",
     "Compared with a single notebook instance as the worker, using this feature:\n",
     "- With higher capacity, you are unlikely to run into OOM or run out of disk space.\n",
     "- With higher parallelism, you can inspect the results much faster and have a better interactive experience.\n",
     "\n",
     "We'll go through 3 examples:\n",
     "- A modified word count to ease into the configurations needed\n",
     "- Process tens of millions flight records (~1GB) to find out how many are delayed for each airline\n",
     "- Classify 50,000 (~280GB) images\n",
     "\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a9140415-f492-4301-a773-de69ff0dc47f",
    "metadata": {},
    "source": [
     "## Prerequisites\n",
     "\n",
     "- The usage of `FlinkRunner` on Cloud Dataproc is supported since Beam v2.40.0.\n",
     "- The `FlinkRunner` is implicitly hosted on an automatically started and managed Cloud Dataproc cluster based on `GoogleCloudOptions` and `WorkerOptions`. Thus:\n",
     "  - Cloud Dataproc product/API needs to be enabled\n",
     "  - The authed user/SA needs to have permissions (`roles/dataproc.admin` or `roles/dataproc.editor`) to manipulate clusters and jobs."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "48ccbadb-80aa-47fb-8f6d-8f84e4fbdddd",
    "metadata": {},
    "outputs": [],
    "source": [
     "from apache_beam.version import __version__\n",
     "print(f'Beam version is: {__version__}')\n",
     "print()\n",
     "\n",
     "print('Authenticated account is: ')\n",
     "!gcloud config get account\n",
     "print()\n",
     "\n",
     "!gcloud services list | grep -q dataproc && echo \"Cloud Dataproc Enabled\" || echo \"Cloud Dataproc Not enabled\"\n",
     "print()\n",
     "\n",
     "print('Granted roles are: ')\n",
     "!gcloud projects get-iam-policy \"$(gcloud config get project)\" --flatten=\"bindings[].members\" --format=\"table(bindings.members, bindings.role)\" --filter=\"bindings.members:$(gcloud config get account)\""
    ]
   },
   {
    "cell_type": "markdown",
    "id": "dc70ee71-0941-4678-9ffa-4835f4850f15",
    "metadata": {
     "tags": []
    },
    "source": [
     "## Basic Imports"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "039291e1-60e8-43e2-a3b8-07cbf74fccfc",
    "metadata": {},
    "outputs": [],
    "source": [
     "from apache_beam.options.pipeline_options import FlinkRunnerOptions\n",
     "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
     "from apache_beam.options.pipeline_options import PipelineOptions\n",
     "from apache_beam.options.pipeline_options import PortableOptions\n",
     "from apache_beam.options.pipeline_options import SetupOptions\n",
     "from apache_beam.options.pipeline_options import WorkerOptions\n",
     "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
     "from apache_beam.runners.portability.flink_runner import FlinkRunner\n",
     "\n",
     "import logging\n",
     "logging.getLogger().setLevel(logging.ERROR)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "83c09c35-d46a-4839-8ffd-279f1903df4f",
    "metadata": {},
    "source": [
     "## Detect the current project\n",
     "We will use the default project configured in the current environment."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "d0014a98-cd7e-422b-b664-4f17aec61bf6",
    "metadata": {},
    "outputs": [],
    "source": [
     "import google.auth\n",
     "project = google.auth.default()[1]"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "7857d346-cb0d-4805-ac7e-568fe32e0592",
    "metadata": {},
    "source": [
     "## Set a distributed cache directory\n",
     "We can specify a Cloud Storage bucket to cache our pipeline results to as we compute on a distributed cluster."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "533d8ffa-ecac-4b11-b89a-ab0134f17d83",
    "metadata": {},
    "outputs": [],
    "source": [
     "# IMPORTANT! Adjust the following to choose a Cloud Storage location.\n",
     "ib.options.cache_root = 'gs://<YOUR-GCS-BUCKET>/flink'"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "ce039c0a-3929-4228-b31e-637f1242c970",
    "metadata": {},
    "source": [
     "## Create an interactive FlinkRunner and configure pipeline options\n",
     "Instead of a base `InteractiveRunner` with default `PipelineOptions`, this notebook sets the `underlying_runner` to `FlinkRunner` and configures Google Cloud and Worker specific pipeline options."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "e5fcc1ee-497e-4298-bf08-8efaaef25338",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Define an InteractiveRunner that uses the FlinkRunner under the hood.\n",
     "interactive_flink_runner = InteractiveRunner(underlying_runner=FlinkRunner())\n",
     "\n",
     "# Set up the Apache Beam pipeline options.\n",
     "options = PipelineOptions()\n",
     "options.view_as(GoogleCloudOptions).project = project\n",
     "# Use cloudpickle to alleviate the burden of staging things in the main module.\n",
     "options.view_as(SetupOptions).pickle_library = 'cloudpickle'\n",
     "# As a rule of thumb, the Flink cluster has about vCPU * #TMs = 8 * 40 = 320 slots.\n",
     "options.view_as(WorkerOptions).machine_type = 'n1-highmem-8'\n",
     "options.view_as(WorkerOptions).num_workers = 40"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "05279582-a433-4a80-878d-f5bcfe9dbc72",
    "metadata": {},
    "source": [
     "## Example 1 - Word Count\n",
     "\n",
     "The cells below contain a modified version of the sample code provided in the [01-Word_Count](01-Word_Count.ipynb) example. "
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "5b974a6d-27fb-4ed4-a892-1571237dfabc",
    "metadata": {},
    "outputs": [],
    "source": [
     "import re\n",
     "\n",
     "\n",
     "class ReadWordsFromText(beam.PTransform):\n",
     "    def __init__(self, file_pattern):\n",
     "        self._file_pattern = file_pattern\n",
     "    \n",
     "    def expand(self, pcoll):\n",
     "        return (pcoll.pipeline\n",
     "                | beam.io.ReadFromText(self._file_pattern)\n",
     "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "23e895b5-e94b-458f-a525-f09f7710c0bd",
    "metadata": {},
    "source": [
     "### Run the Word Count example\n",
     "\n",
     "The first time running a pipeline with the `FlinkRunner` will take longer than usual because it takes time to start and provision the underlying Cloud Dataproc cluster with workers. Later executions reusing the same cluster will be faster."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "5cf0fadc-5380-4c57-a4ca-57ff06668249",
    "metadata": {},
    "outputs": [],
    "source": [
     "p_word_count = beam.Pipeline(interactive_flink_runner, options=options)\n",
     "\n",
     "counts = (\n",
     "    p_word_count\n",
     "    | 'read' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')\n",
     "    | 'count' >> beam.combiners.Count.PerElement())"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "26f07dcf-50e4-41ba-86aa-10bbca7621fe",
    "metadata": {},
    "outputs": [],
    "source": [
     "ib.show(counts)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "6c2dbde7-deea-41fe-96eb-be6b671a53c8",
    "metadata": {},
    "source": [
     "### Inspect the underlying cluster and Flink dashboard\n",
     "To see more information regarding the pipeline run and the Flink cluster, we can describe a cluster used by a pipeline."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "67c231e9-ce0e-4833-b680-667ab4fa4785",
    "metadata": {},
    "outputs": [],
    "source": [
     "ib.clusters.describe(p_word_count)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "54b84fb7-a10c-40be-bbbf-bcc6803c25d3",
    "metadata": {
     "tags": []
    },
    "source": [
     "## FYI: Reuse an existing cluster managed by Interactive Beam\n",
     "\n",
     "- By default, Interactive Beam **always reuses** the most recently used cluster to run a pipeline with the `FlinkRunner` if no pipeline options are given.\n",
     "    - To avoid this behavior, e.g. running another pipeline in the same notebook session with a FlinkRunner not hosted by the notebook, run `ib.clusters.set_default_cluster(None)`.\n",
     "- When instantiating a new pipeline that uses a project, region and provisioning configuration which map to an existing Dataproc cluster, we will also reuse the cluster (might not be the most recently used though).\n",
     "- However, whenever a provisioning change (e.g. resizing a cluster) is given, a new cluster will be created to actuate the desired change. Be aware to avoid exhausting  cloud resources by cleaning up unnecessary clusters through `ib.clusters.cleanup(pipeline)` if resizing a cluster is intended."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "700feeef-92fc-441f-9ab1-2acd30019e01",
    "metadata": {},
    "source": [
     "## Example 2 - Find out how many flights are delayed\n",
     "\n",
     "The example reads more than 17 million records from a public BigQuery dataset `bigquery-samples.airline_ontime_data.flights` and counts how many flights have been delayed since 2010 for all the airlines.\n",
     "\n",
     "The data is considered \"large\" not because of the total size (~1GB) but quantity of rows to read from BigQuery.\n",
     "\n",
     "This usually takes more than 1 hour for a single worker. Here we explicitly define a higher parallelism (150) to execute it and speed up the process (~4mins) reusing the existing cluster."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "36f7f72d-7116-46c6-a53e-05286f4606be",
    "metadata": {},
    "source": [
     "### Setup requirements\n",
     "\n",
     "For the flights example, you need to activate BigQuery service to read data."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "a0b0cac8-c441-4be5-ac4a-89994d1fbe7d",
    "metadata": {},
    "outputs": [],
    "source": [
     "!gcloud services enable bigquery.googleapis.com"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "53170946-df14-4217-b904-f1dd14933feb",
    "metadata": {},
    "source": [
     "### Run the flights example"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "35772c3b-dced-4d51-b06d-5b18bd49c042",
    "metadata": {},
    "outputs": [],
    "source": [
     "# The parallelism is applied to each step, so if your pipeline has 10 steps, you\n",
     "# end up having 150 * 10 tasks scheduled that can be executed in parallel by\n",
     "# the 320 (upper bound) slots/workers/threads theoretically.\n",
     "options.view_as(FlinkRunnerOptions).parallelism = 150"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "43e360a4-34b8-48ef-be20-1c49d9fdca5d",
    "metadata": {},
    "outputs": [],
    "source": [
     "# The BQ read needs a Cloud Storage bucket as a temporary location.\n",
     "options.view_as(GoogleCloudOptions).temp_location = ib.options.cache_root\n",
     "p_bq = beam.Pipeline(runner=interactive_flink_runner, options=options)\n",
     "\n",
     "delays_by_airline = (\n",
     "    p_bq\n",
     "    | 'Read Dataset from BQ' >> beam.io.ReadFromBigQuery(\n",
     "        project=project, use_standard_sql=True,\n",
     "        query=('SELECT airline, arrival_delay '\n",
     "               'FROM `bigquery-samples.airline_ontime_data.flights` '\n",
     "               'WHERE date >= \"2010-01-01\"'))\n",
     "    | 'Rebalance Data to TM Slots' >> beam.Reshuffle(num_buckets=1000)\n",
     "    | 'Extract Delay Info' >> beam.Map(\n",
     "        lambda e: (e['airline'], e['arrival_delay'] > 0))\n",
     "    | 'Filter Delayed' >> beam.Filter(lambda e: e[1])\n",
     "    | 'Count Delayed Flights Per Airline' >> beam.combiners.Count.PerKey())"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "f934a41c-c20b-4a33-8b13-1157f8a29aed",
    "metadata": {},
    "outputs": [],
    "source": [
     "# With visualize_data=True, in the rendered facets widget, you can bin airline by counts\n",
     "# and find out that WN airline has the most delayed flights in the sampled records.\n",
     "ib.show(delays_by_airline, visualize_data=True)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "845a9bff-4e14-493c-8d6a-ecad4876978d",
    "metadata": {},
    "source": [
     "Similarly, we can display information about the cluster used by this new pipeline. Note that the `master_url` and `dashboard` values are the same as the first pipeline (`p_word_count`). This is because the cluster configuration from pipeline options is still the same. If a different region, number of workers or machine type is used, a new cluster would be created."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "73a7a285-d303-4688-8e2c-16df76dd8614",
    "metadata": {},
    "outputs": [],
    "source": [
     "ib.clusters.describe(p_bq)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "156e9974-ae0b-4a0a-a693-b0ab9db14bb0",
    "metadata": {
     "tags": []
    },
    "source": [
     "## Example 3 - Computer Vision: run inference to classify images\n",
     "The RunInference example classifies 50,000 image files (~280GB) from within the notebook.\n",
     "\n",
     "**Disclaimer**: The example uses the validation image set from ImageNet and the PyTorch pre-trained ImageNetV2 model.\n",
     "You can download similar dependencies or use your own image dataset and pre-trained model.\n",
     "Due to restrictions of usage policies and copyrights, we don't have these public datasets hosted on Google Cloud.\n",
     "\n",
     "\n",
     "This example usually takes half a day for a single worker. Here we further increase the parallelism to speed up (~1min)."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "dfb5d501-abbf-4ae7-b028-d0595527d0f9",
    "metadata": {},
    "source": [
     "### Setup requirements\n",
     "\n",
     "For the RunInference example, you need to use Cloud Build to build a container image and store it in Container Registry."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "3fac9815-cd83-4fe9-b918-9f24b03db980",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Enable the cloud build service\n",
     "!gcloud services enable cloudbuild.googleapis.com\n",
     "\n",
     "# Enable the container registry service\n",
     "!gcloud services enable containerregistry.googleapis.com"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "fd1c3aa0-ad84-4801-98ce-6815e479381d",
    "metadata": {},
    "source": [
     "### Build a customer container\n",
     "Normally, if your pipeline doesn’t require additional Python dependencies or executables, Beam automatically uses its official container images. It comes with many common Python modules and you don’t have to build or explicitly specify it.\n",
     "\n",
     "For this example, you are going to use a few extra Python dependencies and a pre-trained model, so you have to build it and make it available for the Flink cluster for execution. The advantages of using a custom container are:\n",
     "Faster setup time for consecutive/interactive executions\n",
     "Stable configurations/dependencies\n",
     "More flexibility: you can set up more than Python dependencies"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "179fb8fd-25e5-425b-a952-fb4369b39682",
    "metadata": {},
    "outputs": [],
    "source": [
     "!mkdir -p /home/jupyter/.flink\n",
     "\n",
     "# IMPORTANT! Adjust to download or copy your model to the directory. The example uses MobileNetV2.\n",
     "!cp /path/to/your-pre-trained-model /home/jupyter/.flink/mobilenet_v2.pt"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "3fc4dd7b-5739-4729-a3a4-a5aed037745c",
    "metadata": {},
    "source": [
     "Install the extra Python dependencies."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "1988d183-aa28-45e0-b20c-3527309c2f3f",
    "metadata": {},
    "outputs": [],
    "source": [
     "%pip install torch\n",
     "%pip install torchvision\n",
     "%pip install pillow\n",
     "%pip install transformers"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "303b295f-8d46-4827-bd7d-d20d067f304c",
    "metadata": {},
    "source": [
     "Export your dependencies into a requirements file.\n",
     "\n",
     "- You can either explicitly create a requirements file with the %%writefile notebook magic.\n",
     "- Or freeze all local dependencies into a requirements file (might introduce unintended deps)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "aded70a3-ac36-469a-a95f-eaacd9cbeb9e",
    "metadata": {},
    "outputs": [],
    "source": [
     "%%writefile /home/jupyter/.flink/requirements.txt\n",
     "torch\n",
     "torchvision\n",
     "pillow\n",
     "transformers"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "aefd3379-332a-4eaa-b03d-931d8b0e992d",
    "metadata": {},
    "source": [
     "And create a Dockerfile with the %%writefile notebook magic.\n",
     "The custom container uses the image of Beam v2.40.0 with Python3.7 as the base, additionally adds a pre-trained MobileNetV2 PyTorch model, and installs the dependencies."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "03c03f44-b26d-40cb-a12a-17c7e60f48d5",
    "metadata": {},
    "outputs": [],
    "source": [
     "%%writefile /home/jupyter/.flink/Dockerfile\n",
     "FROM apache/beam_python3.7_sdk:2.40.0\n",
     "\n",
     "COPY  requirements.txt /tmp/requirements.txt\n",
     "COPY  mobilenet_v2.pt /tmp/mobilenet_v2.pt\n",
     "RUN python -m pip install -r /tmp/requirements.txt"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "4b60d97d-0471-4426-aa7f-fe19bd62520d",
    "metadata": {},
    "source": [
     "Finally, use Cloud Build (do not build the container image on the notebook instance itself) to build the container image and save it to the Container Registry."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "11e6875f-69e4-4814-b5fd-1b80fecd9eb2",
    "metadata": {},
    "outputs": [],
    "source": [
     "!cd /home/jupyter/.flink \\\n",
     " && gcloud builds submit \\\n",
     "     --tag gcr.io/$(gcloud config get-value project)/flink:latest \\\n",
     "     --timeout=20m\n",
     "\n",
     "# Use the custom container you just built.\n",
     "options.view_as(PortableOptions).environment_config = f'gcr.io/{project}/flink'"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "594deb4d-77f7-4738-be24-fddd33f8697e",
    "metadata": {},
    "source": [
     "### Build the pipeline and inspect results"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "869a0206-61be-4f0b-a819-52d0486f60bf",
    "metadata": {},
    "outputs": [],
    "source": [
     "import io\n",
     "from typing import Iterable\n",
     "from typing import Optional\n",
     "from typing import Tuple\n",
     "\n",
     "import torch\n",
     "from PIL import Image\n",
     "from torchvision import models\n",
     "from torchvision import transforms\n",
     "from torchvision.models.mobilenetv2 import MobileNetV2\n",
     "\n",
     "import apache_beam as beam\n",
     "from apache_beam.io.filesystems import FileSystems\n",
     "from apache_beam.ml.inference.base import KeyedModelHandler\n",
     "from apache_beam.ml.inference.base import PredictionResult\n",
     "from apache_beam.ml.inference.base import RunInference\n",
     "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "15fc1433-7589-4921-ab4b-c85d4ff14b20",
    "metadata": {},
    "outputs": [],
    "source": [
     "def filter_empty_text(text: str) -> Iterable[str]:\n",
     "  if len(text.strip()) > 0:\n",
     "    yield text\n",
     "\n",
     "def preprocess_image(data: Image.Image) -> torch.Tensor:\n",
     "  image_size = (224, 224)\n",
     "  # Pre-trained PyTorch models expect input images normalized with the\n",
     "  # below values (see: https://pytorch.org/vision/stable/models.html)\n",
     "  normalize = transforms.Normalize(\n",
     "      mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
     "  transform = transforms.Compose([\n",
     "      transforms.Resize(image_size),\n",
     "      transforms.ToTensor(),\n",
     "      normalize,\n",
     "  ])\n",
     "  return transform(data)\n",
     "\n",
     "def read_image(image_file_name: str) -> Tuple[str, torch.Tensor]:\n",
     "  with FileSystems().open(image_file_name, 'r') as file:\n",
     "    data = Image.open(io.BytesIO(file.read())).convert('RGB')\n",
     "    return image_file_name, preprocess_image(data)\n",
     "\n",
     "class PostProcessor(beam.DoFn):\n",
     "  def process(self, element: Tuple[str, PredictionResult]) -> Iterable[str]:\n",
     "    filename, prediction_result = element\n",
     "    prediction = torch.argmax(prediction_result.inference, dim=0)\n",
     "    yield str(prediction.item())"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "bb622b7a-3f8e-4822-9f0e-5f00f3ba1b51",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Replace this with a file containing paths to your image files.\n",
     "image_file_names = 'gs://runinference/it_mobilenetv2_imagenet_validation_inputs.txt'\n",
     "model_state_dict_path = '/tmp/mobilenet_v2.pt'\n",
     "model_class = MobileNetV2\n",
     "model_params = {'num_classes': 1000}\n",
     "\n",
     "# In this example we pass keyed inputs to RunInference transform.\n",
     "# Therefore, we use KeyedModelHandler wrapper over PytorchModelHandler.\n",
     "model_handler = KeyedModelHandler(\n",
     "  PytorchModelHandlerTensor(\n",
     "      state_dict_path=model_state_dict_path,\n",
     "      model_class=model_class,\n",
     "      model_params=model_params))"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "6c9ca4ff-1792-4f7f-a73a-ce43ec976d01",
    "metadata": {},
    "outputs": [],
    "source": [
     "p_computer_vision = beam.Pipeline(interactive_flink_runner, options=options)\n",
     "\n",
     "counts = (\n",
     "    p_computer_vision\n",
     "    | 'Read Image File Names' >> beam.io.ReadFromText(\n",
     "        image_file_names)\n",
     "    | 'Filter Empty File Names' >> beam.ParDo(filter_empty_text)\n",
     "    | 'Shuffle Files to Read' >> beam.Reshuffle(num_buckets=900)\n",
     "    | 'Read Image Data' >> beam.Map(read_image)\n",
     "    | 'PyTorch Run Inference' >> RunInference(model_handler)\n",
     "    | 'Process Output' >> beam.ParDo(PostProcessor())\n",
     "    | 'Count Per Classification' >> beam.combiners.Count.PerElement())\n",
     "\n",
     "# Further increase the parallelism from the starter example.\n",
     "options.view_as(FlinkRunnerOptions).parallelism = 300"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "118e81b8-bdcb-49e1-a046-01febea11dc2",
    "metadata": {},
    "outputs": [],
    "source": [
     "ib.collect(counts)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "9d620e2e-a671-4225-abf6-0d2cb052ce61",
    "metadata": {},
    "source": [
     "### Enrich the data\n",
     "\n",
     "We can enrich the data with some human-readable labels."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "439ff027-bdb3-4534-9328-40ca23c923fa",
    "metadata": {},
    "outputs": [],
    "source": [
     "idx_to_label = p_computer_vision | 'A sample class idx to label' >> beam.Create(list({\n",
     "    '242': 'boxer',\n",
     "    '243': 'bull mastiff',\n",
     "    '244': 'Tibetan mastiff',\n",
     "    '245': 'French bulldog',\n",
     "    '246': 'Great Dane',\n",
     "    '247': 'Saint Bernard, St Bernard',\n",
     "    '248': 'Eskimo dog, husky',\n",
     "    '249': 'malamute, malemute, Alaskan malamute',\n",
     "    '250': 'Siberian husky',\n",
     "    '251': 'dalmatian, coach dog, carriage dog',\n",
     "    '252': 'affenpinscher, monkey pinscher, monkey dog',\n",
     "    '253': 'basenji',\n",
     "    '254': 'pug, pug-dog',\n",
     "}.items()))\n",
     "\n",
     "def cross_join(idx_count, idx_labels):\n",
     "  idx, count = idx_count\n",
     "  if idx in idx_labels:\n",
     "    return {'class': idx, 'label': idx_labels[idx], 'count': count}\n",
     "\n",
     "label_counts = (\n",
     "    counts\n",
     "    | 'Enrich with human-readable labels' >> beam.Map(\n",
     "        cross_join, idx_labels=beam.pvalue.AsDict(idx_to_label))\n",
     "    | 'Keep only enriched data' >> beam.Filter(lambda x: x is not None))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a93d1247-b418-42ee-acac-2ff8a0d65e84",
    "metadata": {},
    "source": [
     "After an aggregation, the output data size can be tiny compared with the input data.\n",
     "\n",
     "High parallelism does not help with processing small data and could introduce unnecessary overhead.\n",
     "\n",
     "We can interactively tune down the parallelism (1) to inspect the result of the newly added transform that only processes a handful of elements."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "cfe6ed65-c03d-464b-8549-e28cfa717261",
    "metadata": {},
    "outputs": [],
    "source": [
     "options.view_as(FlinkRunnerOptions).parallelism = 1\n",
     "ib.show(label_counts)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "846174c8-d751-4af8-a7b3-39a4ab53846d",
    "metadata": {},
    "source": [
     "`p_computer_vision` reuses the same notebook-managed cluster as the first two pipelines."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "7c370021-f008-4dbb-b361-75236be8b991",
    "metadata": {},
    "outputs": [],
    "source": [
     "ib.clusters.describe(p_computer_vision)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "716544f7-d2fb-464f-84ae-0ff008c4e2fe",
    "metadata": {},
    "source": [
     "### Cleanup\n",
     "Once we are done, we can cleanup the clusters. This will delete the created Dataproc cluster, the staging files of the cluster, and all of the mappings pertaining to the cluster."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "4d446af2-4aae-411c-a52d-a230fa212046",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Use force=True to clean up all notebook-managed clusters.\n",
     "ib.clusters.cleanup(force=True)"
    ]
   }
  ],
  "metadata": {
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.7.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
 