{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "394e1cd2",
      "metadata": {},
      "source": [
        "##### Copyright 2021 Google Inc.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "\u003c!--\n",
        "    Licensed to the Apache Software Foundation (ASF) under one\n",
        "    or more contributor license agreements.  See the NOTICE file\n",
        "    distributed with this work for additional information\n",
        "    regarding copyright ownership.  The ASF licenses this file\n",
        "    to you under the Apache License, Version 2.0 (the\n",
        "    \"License\"); you may not use this file except in compliance\n",
        "    with the License.  You may obtain a copy of the License at\n",
        "\n",
        "      http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Unless required by applicable law or agreed to in writing,\n",
        "    software distributed under the License is distributed on an\n",
        "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "    KIND, either express or implied.  See the License for the\n",
        "    specific language governing permissions and limitations\n",
        "    under the License.\n",
        "--\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f8b934-ed50-482e-bf36-fa65c766b990",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Beam SQL in notebooks\n",
        "\n",
        "This example demonstrates how to use SQL to write Apache Beam pipelines, use the `beam_sql` magic (since Beam v2.34.0) to quickly iterate the pipeline development in notebooks, and launch Dataflow jobs with Beam SQL from notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6783e4b-7f64-44fc-8734-fba455d3dcce",
      "metadata": {},
      "source": [
        "## An overview of Beam SQL\n",
        "\n",
        "[Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) allows a Beam user (currently only available in Beam Java and Python) to query bounded and unbounded PCollections with SQL statements. Your SQL query is translated to a PTransform, an encapsulated segment of a Beam pipeline. You can freely mix SQL PTransforms and other PTransforms in your pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52d1287-7737-4923-a7c9-d4b29008ebc1",
      "metadata": {},
      "source": [
        "Since SQL support in Beam Python SDK is implemented through xLang external transform, make sure you have below prerequisites:\n",
        "- Have `docker` installed;\n",
        "- Have jdk8 or jdk11 installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4a6b21-cc5c-47f2-82d8-e4ec44d143de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The notebook environment should have docker and jdk 1.8 installed.\n",
        "!docker image list\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5d5c83-d14e-4787-a850-de14f4075f48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally sets the logging level to reduce distraction.\n",
        "import logging\n",
        "\n",
        "logging.root.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c15613-0722-4307-8fcf-81b490a3b68f",
      "metadata": {},
      "source": [
        "**Important**: If you're using Beam built from your local source code, additionally:\n",
        "\n",
        "- Have the Java expansion service shadowjar built. Go to the root directory of your local beam repo and then execute:\n",
        "  `./gradlew :sdks:java:extensions:sql:expansion-service:shadowJar`.\n",
        "- Based on your jdk version, pull the docker image `docker pull apache/beam_java11_sdk` or `docker pull apache/beam_java8_sdk`.\n",
        "- Then tag the image with your current Beam dev version.  You can check the dev version under `apache_beam.version.__version__`. For example, if you're using jdk11 and dev version is `x.x.x.dev`, execute `docker image tag apache/beam_java11_sdk:latest apache/beam_java11_sdk:x.x.x.dev`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ba8473-a6be-4ffd-9707-281ea6614bcb",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Run Beam SQL in notebooks with `beam_sql` magic\n",
        "\n",
        "[Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) allows a Beam user to query PCollections with SQL statements. Currently, `InteractiveRunner` does not support `SqlTransform` due to [BEAM-10708](https://issues.apache.org/jira/browse/BEAM-10708). However, a user could use the `beam_sql` magic to run Beam SQL in the notebook and introspect the result. \n",
        "\n",
        "`beam_sql` is an IPython [custom magic](https://ipython.readthedocs.io/en/stable/config/custommagics.html). If you're not familiar with magics, here are some [built-in examples](https://ipython.readthedocs.io/en/stable/interactive/magics.html). It's a convenient way to validate your queries locally against known/test data sources when prototyping a Beam pipeline with SQL, before productionizing it on remote cluster/services.\n",
        "\n",
        "The notebook environment has preloaded the `beam_sql` magic. You can also explicitly load it via `%load_ext apache_beam.runners.interactive.sql.beam_sql_magics` if you set up your own notebook elsewhere.\n",
        "\n",
        "**Note**: `beam_sql` is for `apache_beam[interactive]\u003e=2.34.0`. Make sure to connect your notebook to a kernel that meets the requirement.\n",
        "\n",
        "The `beam_sql` magic can be used as either a line magic or a cell magic. You can check its usage by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfb9966",
      "metadata": {},
      "outputs": [],
      "source": [
        "%beam_sql -h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf321d12-546d-47e3-be43-00dff46ee79d",
      "metadata": {},
      "source": [
        "The advantages of using `beam_sql` in notebooks are:\n",
        "- all scenarios use the same intuitive syntax and you don't have to differentiate them from each other any more\n",
        "    - no need to use the constant `PCOLLECTION` when querying a single PCollection\n",
        "    - no need to name multiple input PCollections, instead referring them by their variable names\n",
        "- no need to write SqlTransform and other Beam related boilerplates\n",
        "- can introspect the result immediately without running the pipeline explicitly or implicitily through a context manager\n",
        "- automatically handles coder registration for your PCollection schemas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d3535b-75bb-44e5-8c24-f1a85b208e25",
      "metadata": {
        "tags": []
      },
      "source": [
        "### There are three scenarios for Beam SQL\n",
        "\n",
        "When writing Beam SQL, this notebook uses the `beam_sql` magic because of its advantages over the `SqlTransform`. *A comparison between using the `beam_sql` magic and `SqlTransform` to write Beam SQL can be found in the [appendix](#Beam-SQL-without-beam_sql-magic)*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "497237cb-0173-46e7-9332-4a87d0068409",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### 1. Use Beam SQL to create a PCollection from constant values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb51474e-c974-4574-9b8e-7f5189832038",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o pcoll\n",
        "SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c71499f-8892-42cb-acb4-40d336d4d252",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### 2. Use Beam SQL to query a PCollection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c650397-78ca-49b3-bf74-42e642c41725",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o id_pcoll\n",
        "SELECT id FROM pcoll"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a31af31-ed51-4a32-acf7-61b72a1d2ec8",
      "metadata": {},
      "source": [
        "#### 3. Use Beam SQL to query multiple PCollections (joined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9af0bce-ecbe-4111-8463-00bd77f5f230",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o value_with_same_id\n",
        "SELECT * FROM pcoll JOIN id_pcoll USING (id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba5d714-4d12-4346-a821-66ffe19dae4d",
      "metadata": {},
      "source": [
        "## Example queries\n",
        "\n",
        "Now that you are familiar with the Beam SQL and the `beam_sql` magic, let's view a few more examples with advanced usage of the `beam_sql` magic such as mixing it with other Beam I/O connectors.\n",
        "\n",
        "### Query#1 - A simple static query\n",
        "\n",
        "You can run a simple SQL query (in Apache Calcite SQL [syntax](https://beam.apache.org/documentation/dsls/sql/calcite/query-syntax/)) to create a [schema-aware PCollection](https://beam.apache.org/documentation/programming-guide/#schemas) from constant values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee6764a-674e-4454-ac47-c510a46bc21f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o query1_data\n",
        "SELECT CAST(5 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563f298c-6151-44cf-9c6d-00927d725062",
      "metadata": {},
      "source": [
        "The `beam_sql` magic shows you the result of the SQL query.\n",
        "\n",
        "It also creates and outputs a PCollection named `query1_data` with `element_type` like `BeamSchema_...(id: int32, str: str)`.\n",
        "\n",
        "Note that you have **not** explicitly created a Beam pipeline. You get a PCollection because the `beam_sql` magic always **implicitly creates** a pipeline to execute your SQL query. To hold the elements with each field's type info, Beam automatically creates a schema as the `element_type` for the created PCollection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558537ff-f2f4-498b-b014-40c378f069f3",
      "metadata": {},
      "source": [
        "To introspect the data again with more knobs, you can use `show`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c26e43-9a7a-4a9b-b39b-9a6cfccae532",
      "metadata": {},
      "outputs": [],
      "source": [
        "from apache_beam.runners.interactive import interactive_beam as ib\n",
        "ib.show(query1_data)\n",
        "# Uncomment below to set more args.\n",
        "# ib.show(query1_data, visualize_data=True, include_window_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcc722d-da81-4fc2-b0c0-3b86319ce03a",
      "metadata": {},
      "source": [
        "To materialize the PCollection into a pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe) object, you can use `collect`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f105733-5ecd-4d37-89c7-173769283986",
      "metadata": {},
      "outputs": [],
      "source": [
        "ib.collect(query1_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c581e90-e636-4bca-80d8-069743b2c631",
      "metadata": {},
      "source": [
        "You can also additionally append some transforms such as writing to a text file and print the elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45724257-6ba0-4bd8-a0c8-95e19dd81546",
      "metadata": {},
      "outputs": [],
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "coder=beam.coders.registry.get_coder(query1_data.element_type)\n",
        "print(coder)\n",
        "query1_data | beam.io.textio.WriteToText('/tmp/query1_data', coder=coder)\n",
        "query1_data | beam.Map(print)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdaf489-0c9e-4eb3-817e-96c052774fd5",
      "metadata": {},
      "source": [
        "Execute the pipeline as a normal pipeline running on DirectRunner and inspect the output file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebfbe6c-0126-4157-8d42-b2893c8f4d26",
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /tmp/query1_data*\n",
        "query1_data.pipeline.run().wait_until_finish()\n",
        "!ls /tmp/query1_data*\n",
        "!cat /tmp/query1_data*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416d317a-aed2-451a-b5c6-626878bda0f5",
      "metadata": {},
      "source": [
        "The coder in use is a `RowCoder`. The element is encoded and written to the text file. When inspecting it directly, it may display **garbled strings**. The file will be revisited later in Query#4."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8159b88-c79c-4d77-8966-388fc014f608",
      "metadata": {},
      "source": [
        "### [Optional] Omit the `-o` option.\n",
        "If the option is omitted, an output name is auto-generated based on the SQL query and PCollection (if any) it queries. Optionally, you can also use the `_[{execution_count}]` convention: `_` for last output and `_{execution_count}` for a specific cell execution output.\n",
        "\n",
        "However, explicitly naming the output is recommended for better notebook readability and to avoid unexpected errors.\n",
        "\n",
        "Below example outputs a PCollection named like `sql_output_...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423ead5f-3de0-46a0-8e94-d1206390bddb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql\n",
        "SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda16c42-341b-481c-b704-6d9d3c76d048",
      "metadata": {},
      "source": [
        "Now that you are familiar with the `beam_sql` magic, you can build more queries against PCollections.\n",
        "\n",
        "Let's install the `names` package to randomly generate some names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564ade39-dbbf-4cf6-967e-5f853570319f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1bb84a-be9e-4f3b-98fb-c7750e5b19d0",
      "metadata": {},
      "source": [
        "Import all modules needed for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec5c9b4-ddaa-4d2c-968c-a86c430cfdcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import names\n",
        "import typing\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "from apache_beam.runners.interactive import interactive_beam as ib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8af4cd3-8ef3-4108-a76b-2e9006c0a935",
      "metadata": {},
      "source": [
        "Create a pipeline `p` with the `InteractiveRunner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801f535b-0cfc-4e97-a092-0ff9142350f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = beam.Pipeline(InteractiveRunner())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8063b29-11ae-46e4-89db-436c98182b27",
      "metadata": {},
      "source": [
        "Then let's create a schema with `typing.NamedTuple`. Let's call it `Person` with a field `id` and a field `name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a5e466-e5f3-4932-b4a1-b1cffbfa05bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Person(typing.NamedTuple):\n",
        "    id: int\n",
        "    name: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5f277a-e918-4e09-877d-534cdde45f71",
      "metadata": {},
      "source": [
        "With `beam_sql` magic, you can utilize all the Beam I/O connectors (streaming is currently not supported due to `DirectRunner` not supporting streaming pipeline with `SqlTransform`) as source of data, then build a SQL query against all the data and check the output. If needed, you can sink the output following the `WriteToText` example demonstrated above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f73b1e97-83bc-45e1-8d2c-c462bf5208ae",
      "metadata": {},
      "source": [
        "## Query#2 - Querying a single PCollection\n",
        "\n",
        "Let's build a PCollection with 10 random `Person` typed elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e318c928-0192-4f6a-9f55-47593d683b1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "persons = (p \n",
        "           | beam.Create([Person(id=x, name=names.get_full_name()) for x in range(10)]))\n",
        "ib.show(persons)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d883db-bc6a-441d-b380-36d683759d47",
      "metadata": {},
      "source": [
        "You can look for all elements with `id \u003c 5` in `persons` with the below query and assign the output to `persons_id_lt_5`. Also, you can enable `-v` option to see more details about the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981207a6-e36e-4ee4-b330-5f46f0adbacd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o persons_id_lt_5 -v\n",
        "SELECT * FROM persons WHERE id \u003c5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ebac08-f141-4f60-a705-de98a7922fac",
      "metadata": {},
      "source": [
        "With `-v`, if it's the first time running this query, you might see a warning message about\n",
        "\n",
        "```\n",
        "Schema Person has not been registered to use a RowCoder. Automatically registering it by running: beam.coders.registry.register_coder(Person, beam.coders.RowCoder)\n",
        "```\n",
        "\n",
        "The `beam_sql` magic helps registering a `RowCoder` for each schema you define and use whenever it finds one. You can also explicitly run the same code to do so.\n",
        "\n",
        "Note the output element type is `Person(id: int, name: str)` instead of `BeamSchema_...` because you have selected all the fields from a single PCollection of the known type `Person(id: int, name: str)`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8692e2-60b6-45bf-b91c-b4438d38673a",
      "metadata": {},
      "source": [
        "## Query#3 - Joining multiple PCollections\n",
        "\n",
        "You can build a `persons_2` PCollection with a different range of `id`s and `name`s. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e80d164-93bb-4189-985d-8e0ccf4ee8e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "persons_2 = (p \n",
        "             | beam.Create([Person(id=x, name=names.get_full_name()) for x in range(5, 15)]))\n",
        "ib.show(persons_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d429563f-45cf-4369-8213-5eda145cd1fa",
      "metadata": {},
      "source": [
        "Then query for all `name`s from `persons` and `persons_2` with the same `id`s and assign the output to `persons_with_common_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a77317e-fb65-4682-aeca-c98e8f5099bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o persons_with_common_id -v\n",
        "SELECT * FROM persons JOIN persons_2 USING (id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49cd8d0a-126e-437c-b71b-6455c737c479",
      "metadata": {},
      "source": [
        "Note the output element type is now some `BeamSchema_...(id: int64, name: str, name0: str)`. Because you have selected columns from both PCollections, there is no known schema to hold the result. Beam automatically creates a schema and differentiates conflicted field `name` by suffixing `0` to one of them.\n",
        "\n",
        "And since `Person` is already previously registered with a `RowCoder`, there is no more warning about registering it anymore even with `-v`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3a58e6-99e8-4168-97e5-43d7e0a68e8f",
      "metadata": {},
      "source": [
        "## Query#4 - Join multiple PCollections, including I/O."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd84b50-35ef-465d-8e55-003332b642cf",
      "metadata": {},
      "source": [
        "Let's read the file written by Query#1 and use it to join `persons` and `persons_2` to find `name`s with the common `id` in all three of them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe39750-92d6-4344-97c4-f00cb5982a6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the exact same coder used when WriteToText. \n",
        "# Note for Beam \u003e= 2.39.0, no need to explicitly set the output types.\n",
        "# for Beam \u003c 2.39.0, please use this\n",
        "# query1_result_in_file = p | beam.io.ReadFromText(\n",
        "#     '/tmp/query1_data*', coder=coder).with_output_types(\n",
        "#     query1_data.element_type)\n",
        "  \n",
        "query1_result_in_file = p | beam.io.ReadFromText(\n",
        "    '/tmp/query1_data*', coder=coder)\n",
        "\n",
        "# Check all the data sources.\n",
        "ib.show(query1_result_in_file)\n",
        "ib.show(persons)\n",
        "ib.show(persons_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe914a95-4aec-493d-a240-215ce696dfb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o entry_with_common_id\n",
        "\n",
        "SELECT query1_result_in_file.id, persons.name AS `name_1`, persons_2.name AS `name_2`\n",
        "FROM query1_result_in_file JOIN persons ON query1_result_in_file.id = persons.id\n",
        "JOIN persons_2 ON query1_result_in_file.id = persons_2.id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09af0a54-ae5b-4b5d-bbdc-e46f2da6464b",
      "metadata": {},
      "source": [
        "You can also chain another `beam_sql` magic to get just `name_1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d81190-8dcb-4b53-8cfc-d88512360309",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%beam_sql -o name_found\n",
        "SELECT name_1 AS `name` FROM entry_with_common_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e74af9-eaec-4715-9e18-662d899419cb",
      "metadata": {},
      "source": [
        "## Appendix\n",
        "\n",
        "### Beam SQL without `beam_sql` magic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0514c021-c175-4dae-82e1-33e496ede19a",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### 1. Use Beam SQL to create a PCollection from constant values\n",
        "\n",
        "Note the `SqlTransform` is applied to a pipeline not a PCollection, unlike the below 2 scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a421ced-1769-4374-a543-87d7a60fceb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from apache_beam.transforms.sql import SqlTransform\n",
        "\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "    pcoll = p | 'Create pcoll' \u003e\u003e SqlTransform(\"\"\"\n",
        "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
        "    _ = pcoll | beam.io.WriteToText('/tmp/pcoll')\n",
        "\n",
        "!cat /tmp/pcoll*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12113310-45cf-44b5-a846-56d24398e32f",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### 2. Use Beam SQL to query a PCollection\n",
        "\n",
        "You have to use the `PCOLLECTION` constant in the query.\n",
        "\n",
        "Below query selects `id` field from the above `pcoll`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5dce749-0b67-403a-80c4-2cbe86805e2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "with beam.Pipeline() as p:\n",
        "    pcoll = p | 'Create pcoll' \u003e\u003e SqlTransform(\"\"\"\n",
        "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
        "    id_pcoll = pcoll | 'Select id from pcoll' \u003e\u003e SqlTransform(\"\"\"SELECT id FROM PCOLLECTION\"\"\")\n",
        "    _ = id_pcoll | beam.io.WriteToText('/tmp/id')\n",
        "\n",
        "!cat /tmp/id*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490be2c3-eead-45f2-a482-75620a9902ee",
      "metadata": {},
      "source": [
        "#### 3. Use Beam SQL to query multiple PCollections (joined)\n",
        "You can tag PCollections with names and refer them in the query using the tagged names.\n",
        "\n",
        "Below query joins previous 2 PCollections by `id`, should return the original element from `pcoll`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74e6652-cf42-4f5b-ac48-6cfb58468733",
      "metadata": {},
      "outputs": [],
      "source": [
        "with beam.Pipeline() as p:\n",
        "    pcoll = p | 'Create pcoll' \u003e\u003e SqlTransform(\"\"\"\n",
        "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
        "    id_pcoll = pcoll | 'Select id from pcoll' \u003e\u003e SqlTransform(\"\"\"SELECT id FROM PCOLLECTION\"\"\")\n",
        "    value_with_same_id = {'input_1': pcoll, 'input_2': id_pcoll} | 'Join pcolls' \u003e\u003e SqlTransform(\"\"\"\n",
        "        SELECT * FROM input_1 JOIN input_2 USING (id)\"\"\")\n",
        "    _ = value_with_same_id | beam.io.WriteToText('/tmp/same_id')\n",
        "\n",
        "!cat /tmp/same_id*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "toc-autonumbering": false,
    "toc-showcode": true,
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
